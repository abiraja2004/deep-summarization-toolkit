# Example of training/evaluating configs.
# Model used:
model: PBATransformer

# Model's arguments. Empty to use default arguments.
margs:
  num_layers: 6
  emb_size: 300
  dim_m: 512
  dim_i: 2048
  attention: interleaved

# Dataset used:
dataset: RIADataset

# Default RIADataset's arguments.
dargs:
  init:
    directory: ./datasets/ria
    prefix: ria-300
    max_sequence_length: 150
  preprocess:
    pretrain_emb: True
    vocab_size: 30000
    embedding_size: 300
    # Max sequence length for sentecepiece processor
    max_sentence_length: 16384
    workers: 3
    # Whether to use skip-gram for training word2vec
    skip_gramm: False

# Optimizer used (from torch.optim):
optimizer: Adam

# Optimizer arguments
oargs:
  lr: 1.0e-4
  amsgrad: True
  betas: [0.9, 0.98]
  eps: 1.0e-9

# Training arguments:
targs:
  # Prefix of running model configuration:
  prefix: default-pba-transformer
  # Used device
  device: cuda
  epochs: 10
  train_batch_size: 16
  test_batch_size: 16
  checkpoint_interval: 1000
  train_log_interval: 100

model: PBATransformer

margs:
  num_layers: 6
  emb_size: 300
  dim_m: 512
  dim_i: 2048
  attention: interleaved

dataset: RIADataset

dargs:
  init:
    directory: ./datasets/ria
    prefix: ria-sg-300
    max_sequence_length: 150
    parts:
      train: -1
      test: 20000
      dev: 5000
    n_sentences: 5
    rm_strongs: False
  preprocess:
    pretrain_emb: True
    vocab_size: 30000
    embedding_size: 300
    max_sentence_length: 16384
    workers: 3
    skip_gramm: True

optimizer: Adam

oargs:
  lr: 1.0e-4
  amsgrad: True
  betas: [0.9, 0.98]
  eps: 1.0e-9

targs:
  prefix: cont-pbat-il
  device: cuda
  epochs: 10
  train_batch_size: 16
  test_batch_size: 16
  checkpoint_interval: 1000
  train_log_interval: 100
